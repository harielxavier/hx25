interface CallRecording {
  id: string;
  clientId: string;
  jobId: string;
  fileName: string;
  filePath: string;
  duration: number; // seconds
  recordedAt: Date;
  participants: string[];
  callType: 'consultation' | 'planning' | 'review' | 'delivery' | 'other';
  transcription?: Transcription;
  extractedTasks?: ExtractedTask[];
  processed: boolean;
}

interface Transcription {
  id: string;
  text: string;
  segments: TranscriptionSegment[];
  language: string;
  confidence: number;
  processedAt: Date;
  summary: string;
  keyPoints: string[];
  actionItems: string[];
}

interface TranscriptionSegment {
  id: number;
  start: number; // seconds
  end: number; // seconds
  text: string;
  speaker?: string;
  confidence: number;
}

interface ExtractedTask {
  id: string;
  title: string;
  description: string;
  assignee: 'photographer' | 'client' | 'vendor';
  priority: 'low' | 'medium' | 'high' | 'urgent';
  dueDate?: Date;
  category: 'preparation' | 'communication' | 'delivery' | 'payment' | 'logistics' | 'creative';
  status: 'pending' | 'in_progress' | 'completed' | 'cancelled';
  source: {
    callId: string;
    timestamp: number; // seconds in call
    context: string; // surrounding text
  };
  confidence: number; // 0-1 confidence in extraction
  autoGenerated: boolean;
}

interface TaskExtractionRules {
  keywords: string[];
  patterns: RegExp[];
  context_clues: string[];
  priority_indicators: {
    urgent: string[];
    high: string[];
    medium: string[];
    low: string[];
  };
  assignee_indicators: {
    photographer: string[];
    client: string[];
    vendor: string[];
  };
  category_indicators: {
    preparation: string[];
    communication: string[];
    delivery: string[];
    payment: string[];
    logistics: string[];
    creative: string[];
  };
}

interface SpeakerIdentification {
  speakerId: string;
  name: string;
  role: 'photographer' | 'client' | 'vendor' | 'unknown';
  voiceProfile?: VoiceProfile;
}

interface VoiceProfile {
  pitch: number;
  tone: string;
  speed: number;
  accent?: string;
  confidence: number;
}

class CallTranscriptionService {
  private openaiApiKey: string;
  private whisperApiKey: string;
  private taskExtractionRules: TaskExtractionRules;
  
  constructor() {
    this.openaiApiKey = process.env.REACT_APP_OPENAI_API_KEY || '';
    this.whisperApiKey = process.env.REACT_APP_OPENAI_API_KEY || ''; // Same as OpenAI for Whisper
    
    this.taskExtractionRules = {
      keywords: [
        'need to', 'have to', 'should', 'must', 'remember to', 'don\'t forget',
        'action item', 'todo', 'task', 'follow up', 'send', 'call', 'email',
        'schedule', 'book', 'confirm', 'check', 'review', 'prepare', 'deliver'
      ],
      patterns: [
        /I need to (.+)/gi,
        /you need to (.+)/gi,
        /we should (.+)/gi,
        /let me (.+)/gi,
        /I'll (.+)/gi,
        /can you (.+)/gi,
        /please (.+)/gi,
        /action item[:\s]+(.+)/gi,
        /follow up on (.+)/gi,
        /by (.+), we need to (.+)/gi
      ],
      context_clues: [
        'deadline', 'due date', 'by when', 'timeline', 'schedule',
        'urgent', 'asap', 'priority', 'important', 'critical'
      ],
      priority_indicators: {
        urgent: ['urgent', 'asap', 'immediately', 'right away', 'critical'],
        high: ['important', 'priority', 'soon', 'this week'],
        medium: ['should', 'need to', 'when you can'],
        low: ['eventually', 'sometime', 'when convenient']
      },
      assignee_indicators: {
        photographer: ['I will', 'I\'ll', 'let me', 'I need to', 'I should'],
        client: ['you need to', 'can you', 'please', 'you should', 'your task'],
        vendor: ['vendor', 'florist', 'dj', 'caterer', 'planner', 'coordinator']
      },
      category_indicators: {
        preparation: ['prepare', 'setup', 'arrange', 'organize', 'plan'],
        communication: ['call', 'email', 'text', 'contact', 'reach out', 'follow up'],
        delivery: ['deliver', 'send', 'provide', 'share', 'upload', 'gallery'],
        payment: ['payment', 'invoice', 'pay', 'bill', 'cost', 'price'],
        logistics: ['location', 'time', 'schedule', 'coordinate', 'travel'],
        creative: ['style', 'pose', 'shot', 'edit', 'retouch', 'creative']
      }
    };
  }

  /**
   * Process a call recording: transcribe and extract tasks
   */
  async processCallRecording(recording: CallRecording): Promise<{
    transcription: Transcription;
    tasks: ExtractedTask[];
  }> {
    try {
      console.log(`Processing call recording: ${recording.fileName}`);
      
      // Step 1: Transcribe the audio
      const transcription = await this.transcribeAudio(recording);
      
      // Step 2: Identify speakers
      const speakerIdentification = await this.identifySpeakers(transcription, recording);
      
      // Step 3: Extract tasks from transcription
      const tasks = await this.extractTasks(transcription, recording);
      
      // Step 4: Enhance tasks with AI analysis
      const enhancedTasks = await this.enhanceTasksWithAI(tasks, transcription, recording);
      
      // Step 5: Generate summary and key points
      const summary = await this.generateCallSummary(transcription, recording);
      
      const finalTranscription: Transcription = {
        ...transcription,
        summary: summary.summary,
        keyPoints: summary.keyPoints,
        actionItems: summary.actionItems
      };
      
      return {
        transcription: finalTranscription,
        tasks: enhancedTasks
      };
      
    } catch (error) {
      console.error('Error processing call recording:', error);
      throw new Error(`Failed to process call recording: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  /**
   * Transcribe audio using OpenAI Whisper
   */
  private async transcribeAudio(recording: CallRecording): Promise<Transcription> {
    try {
      // Create form data for Whisper API
      const formData = new FormData();
      
      // In a real implementation, you'd read the audio file
      // For now, we'll simulate with a blob
      const audioBlob = await this.getAudioBlob(recording.filePath);
      formData.append('file', audioBlob, recording.fileName);
      formData.append('model', 'whisper-1');
      formData.append('response_format', 'verbose_json');
      formData.append('timestamp_granularities[]', 'segment');

      const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.whisperApiKey}`
        },
        body: formData
      });

      if (!response.ok) {
        throw new Error(`Whisper API error: ${response.status}`);
      }

      const whisperResult = await response.json();
      
      // Convert Whisper response to our format
      const segments: TranscriptionSegment[] = whisperResult.segments?.map((segment: any, index: number) => ({
        id: index,
        start: segment.start,
        end: segment.end,
        text: segment.text.trim(),
        confidence: segment.avg_logprob ? Math.exp(segment.avg_logprob) : 0.8
      })) || [];

      return {
        id: `transcription_${recording.id}`,
        text: whisperResult.text,
        segments,
        language: whisperResult.language || 'en',
        confidence: segments.reduce((sum, seg) => sum + seg.confidence, 0) / segments.length,
        processedAt: new Date(),
        summary: '', // Will be filled later
        keyPoints: [], // Will be filled later
        actionItems: [] // Will be filled later
      };
      
    } catch (error) {
      console.error('Error transcribing audio:', error);
      throw new Error(`Transcription failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }

  /**
   * Get audio blob from file path (mock implementation)
   */
  private async getAudioBlob(filePath: string): Promise<Blob> {
    // In a real implementation, this would read the actual audio file
    // For demo purposes, we'll create a mock blob
    return new Blob(['mock audio data'], { type: 'audio/wav' });
  }

  /**
   * Identify speakers in the transcription
   */
  private async identifySpeakers(
    transcription: Transcription, 
    recording: CallRecording
  ): Promise<SpeakerIdentification[]> {
    // This is a simplified speaker identification
    // In a real implementation, you'd use speaker diarization
    
    const speakers: SpeakerIdentification[] = [
      {
        speakerId: 'speaker_1',
        name: 'Photographer',
        role: 'photographer',
      },
      {
        speakerId: 'speaker_2', 
        name: 'Client',
        role: 'client'
      }
    ];

    // Simple heuristic: assign speakers based on content patterns
    transcription.segments.forEach(segment => {
      if (segment.text.includes('I\'ll take photos') || segment.text.includes('my camera')) {
        segment.speaker = 'Photographer';
      } else if (segment.text.includes('our wedding') || segment.text.includes('we want')) {
        segment.speaker = 'Client';
      }
    });

    return speakers;
  }

  /**
   * Extract tasks from transcription using pattern matching
   */
  private async extractTasks(
    transcription: Transcription, 
    recording: CallRecording
  ): Promise<ExtractedTask[]> {
    const tasks: ExtractedTask[] = [];
    
    // Process each segment for task extraction
    transcription.segments.forEach((segment, index) => {
      const text = segment.text.toLowerCase();
      
      // Check for task patterns
      this.taskExtractionRules.patterns.forEach(pattern => {
        const matches = segment.text.match(pattern);
        if (matches) {
          matches.forEach(match => {
            const task = this.createTaskFromMatch(match, segment, recording, index);
            if (task) {
              tasks.push(task);
            }
          });
        }
      });
      
      // Check for keyword-based tasks
      this.taskExtractionRules.keywords.forEach(keyword => {
        if (text.includes(keyword.toLowerCase())) {
          const task = this.createTaskFromKeyword(keyword, segment, recording, index);
          if (task) {
            tasks.push(task);
          }
        }
      });
    });

    // Remove duplicates and low-confidence tasks
    return this.deduplicateTasks(tasks);
  }

  /**
   * Create task from pattern match
   */
  private createTaskFromMatch(
    match: string, 
    segment: TranscriptionSegment, 
    recording: CallRecording, 
    segmentIndex: number
  ): ExtractedTask | null {
    const taskText = match.replace(/^(I need to|you need to|we should|let me|I'll|can you|please)\s*/i, '');
    
    if (taskText.length < 10) return null; // Too short to be meaningful
    
    const assignee = this.determineAssignee(match);
    const priority = this.determinePriority(segment.text);
    const category = this.determineCategory(taskText);
    const dueDate = this.extractDueDate(segment.text);

    return {
      id: `task_${recording.id}_${segmentIndex}_${Date.now()}`,
      title: this.generateTaskTitle(taskText),
      description: taskText,
      assignee,
      priority,
      dueDate,
      category,
      status: 'pending',
      source: {
        callId: recording.id,
        timestamp: segment.start,
        context: segment.text
      },
      confidence: segment.confidence * 0.8, // Reduce confidence for auto-extraction
      autoGenerated: true
    };
  }

  /**
   * Create task from keyword detection
   */
  private createTaskFromKeyword(
    keyword: string, 
    segment: TranscriptionSegment, 
    recording: CallRecording, 
    segmentIndex: number
  ): ExtractedTask | null {
    // Only create tasks for high-confidence keyword matches
    if (segment.confidence < 0.7) return null;
    
    const sentences = segment.text.split(/[.!?]+/);
    const relevantSentence = sentences.find(sentence => 
      sentence.toLowerCase().includes(keyword.toLowerCase())
    );
    
    if (!relevantSentence || relevantSentence.length < 15) return null;
    
    return {
      id: `task_kw_${recording.id}_${segmentIndex}_${Date.now()}`,
      title: this.generateTaskTitle(relevantSentence),
      description: relevantSentence.trim(),
      assignee: this.determineAssignee(relevantSentence),
      priority: this.determinePriority(relevantSentence),
      category: this.determineCategory(relevantSentence),
      status: 'pending',
      source: {
        callId: recording.id,
        timestamp: segment.start,
        context: segment.text
      },
      confidence: segment.confidence * 0.6, // Lower confidence for keyword-based
      autoGenerated: true
    };
  }

  /**
   * Determine task assignee from text
   */
  private determineAssignee(text: string): ExtractedTask['assignee'] {
    const lowerText = text.toLowerCase();
    
    for (const [assignee, indicators] of Object.entries(this.taskExtractionRules.assignee_indicators)) {
      if (indicators.some(indicator => lowerText.includes(indicator.toLowerCase()))) {
        return assignee as ExtractedTask['assignee'];
      }
    }
    
    return 'photographer'; // Default assignee
  }

  /**
   * Determine task priority from text
   */
  private determinePriority(text: string): ExtractedTask['priority'] {
    const lowerText = text.toLowerCase();
    
    for (const [priority, indicators] of Object.entries(this.taskExtractionRules.priority_indicators)) {
      if (indicators.some(indicator => lowerText.includes(indicator.toLowerCase()))) {
        return priority as ExtractedTask['priority'];
      }
    }
    
    return 'medium'; // Default priority
  }

  /**
   * Determine task category from text
   */
  private determineCategory(text: string): ExtractedTask['category'] {
    const lowerText = text.toLowerCase();
    
    for (const [category, indicators] of Object.entries(this.taskExtractionRules.category_indicators)) {
      if (indicators.some(indicator => lowerText.includes(indicator.toLowerCase()))) {
        return category as ExtractedTask['category'];
      }
    }
    
    return 'preparation'; // Default category
  }

  /**
   * Extract due date from text
   */
  private extractDueDate(text: string): Date | undefined {
    const datePatterns = [
      /by (.+day)/i,
      /before (.+)/i,
      /deadline (.+)/i,
      /due (.+)/i,
      /(monday|tuesday|wednesday|thursday|friday|saturday|sunday)/i,
      /(tomorrow|next week|this week)/i
    ];
    
    for (const pattern of datePatterns) {
      const match = text.match(pattern);
      if (match) {
        // Simple date parsing - in real implementation, use a proper date parser
        const dateStr = match[1];
        if (dateStr.includes('tomorrow')) {
          const tomorrow = new Date();
          tomorrow.setDate(tomorrow.getDate() + 1);
          return tomorrow;
        } else if (dateStr.includes('next week')) {
          const nextWeek = new Date();
          nextWeek.setDate(nextWeek.getDate() + 7);
          return nextWeek;
        } else if (dateStr.includes('this week')) {
          const thisWeek = new Date();
          thisWeek.setDate(thisWeek.getDate() + 3); // Assume mid-week
          return thisWeek;
        }
      }
    }
    
    return undefined;
  }

  /**
   * Generate a concise task title from description
   */
  private generateTaskTitle(description: string): string {
    // Remove common prefixes and clean up
    let title = description
      .replace(/^(I need to|you need to|we should|let me|I'll|can you|please)\s*/i, '')
      .replace(/[.!?]+$/, '')
      .trim();
    
    // Capitalize first letter
    title = title.charAt(0).toUpperCase() + title.slice(1);
    
    // Truncate if too long
    if (title.length > 60) {
      title = title.substring(0, 57) + '...';
    }
    
    return title;
  }

  /**
   * Remove duplicate and low-confidence tasks
   */
  private deduplicateTasks(tasks: ExtractedTask[]): ExtractedTask[] {
    // Sort by confidence (highest first)
    tasks.sort((a, b) => b.confidence - a.confidence);
    
    const uniqueTasks: ExtractedTask[] = [];
    const seenTitles = new Set<string>();
    
    for (const task of tasks) {
      // Skip low-confidence tasks
      if (task.confidence < 0.5) continue;
      
      // Check for similar titles
      const normalizedTitle = task.title.toLowerCase().replace(/[^\w\s]/g, '');
      let isDuplicate = false;
      
      for (const seenTitle of seenTitles) {
        if (this.calculateSimilarity(normalizedTitle, seenTitle) > 0.8) {
          isDuplicate = true;
          break;
        }
      }
      
      if (!isDuplicate) {
        uniqueTasks.push(task);
        seenTitles.add(normalizedTitle);
      }
    }
    
    return uniqueTasks;
  }

  /**
   * Calculate similarity between two strings
   */
  private calculateSimilarity(str1: string, str2: string): number {
    const words1 = str1.split(/\s+/);
    const words2 = str2.split(/\s+/);
    
    const commonWords = words1.filter(word => words2.includes(word));
    const totalWords = new Set([...words1, ...words2]).size;
    
    return commonWords.length / totalWords;
  }

  /**
   * Enhance tasks using AI analysis
   */
  private async enhanceTasksWithAI(
    tasks: ExtractedTask[], 
    transcription: Transcription, 
    recording: CallRecording
  ): Promise<ExtractedTask[]> {
    if (tasks.length === 0) return tasks;
    
    try {
      const prompt = `
Analyze these extracted tasks from a photography consultation call and enhance them:

Call Context: ${recording.callType} call for ${recording.participants.join(', ')}
Transcription Summary: ${transcription.text.substring(0, 500)}...

Extracted Tasks:
${tasks.map((task, i) => `${i + 1}. ${task.title} (${task.assignee}, ${task.priority})`).join('\n')}

Please enhance each task by:
1. Improving the title for clarity
2. Adjusting priority if needed
3. Suggesting better due dates
4. Adding relevant details to description

Return a JSON array with enhanced tasks in this format:
[
  {
    "index": 0,
    "enhanced_title": "improved title",
    "enhanced_description": "more detailed description",
    "suggested_priority": "low|medium|high|urgent",
    "suggested_due_date": "YYYY-MM-DD or null",
    "additional_notes": "any relevant context"
  }
]
`;

      const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.openaiApiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: 'gpt-4',
          messages: [
            {
              role: 'system',
              content: 'You are an expert project manager for photography businesses. Enhance task extraction with practical improvements.'
            },
            {
              role: 'user',
              content: prompt
            }
          ],
          temperature: 0.3,
          max_tokens: 1000
        })
      });

      if (!response.ok) {
        console.warn('AI enhancement failed, returning original tasks');
        return tasks;
      }

      const data = await response.json();
      const enhancements = JSON.parse(data.choices[0].message.content);
      
      // Apply enhancements
      return tasks.map((task, index) => {
        const enhancement = enhancements.find((e: any) => e.index === index);
        if (!enhancement) return task;
        
        return {
          ...task,
          title: enhancement.enhanced_title || task.title,
          description: enhancement.enhanced_description || task.description,
          priority: enhancement.suggested_priority || task.priority,
          dueDate: enhancement.suggested_due_date ? new Date(enhancement.suggested_due_date) : task.dueDate,
          confidence: Math.min(task.confidence + 0.1, 1.0) // Slight confidence boost for AI enhancement
        };
      });
      
    } catch (error) {
      console.warn('AI enhancement failed:', error);
      return tasks; // Return original tasks if enhancement fails
    }
  }

  /**
   * Generate call summary and key points
   */
  private async generateCallSummary(
    transcription: Transcription, 
    recording: CallRecording
  ): Promise<{
    summary: string;
    keyPoints: string[];
    actionItems: string[];
  }> {
    try {
      const prompt = `
Analyze this photography consultation call transcription and provide:

Call Type: ${recording.callType}
Duration: ${Math.round(recording.duration / 60)} minutes
Participants: ${recording.participants.join(', ')}

Transcription:
${transcription.text}

Please provide:
1. A concise summary (2-3 sentences)
2. Key points discussed (bullet points)
3. Action items mentioned (specific tasks)

Return JSON in this format:
{
  "summary": "brief summary of the call",
  "keyPoints": ["point 1", "point 2", "point 3"],
  "actionItems": ["action 1", "action 2", "action 3"]
}
`;

      const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.openaiApiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: 'gpt-4',
          messages: [
            {
              role: 'system',
              content: 'You are an expert at summarizing photography business calls. Be concise and actionable.'
            },
            {
              role: 'user',
              content: prompt
            }
          ],
          temperature: 0.3,
          max_tokens: 800
        })
      });

      if (!response.ok) {
        throw new Error(`OpenAI API error: ${response.status}`);
      }

      const data = await response.json();
      return JSON.parse(data.choices[0].message.content);
      
    } catch (error) {
      console.error('Error generating call summary:', error);
      
      // Fallback summary
      return {
        summary: `${recording.callType} call with ${recording.participants.join(', ')} lasting ${Math.round(recording.duration / 60)} minutes.`,
        keyPoints: ['Call transcribed successfully'],
        actionItems: ['Review transcription for action items']
      };
    }
  }

  /**
   * Get task extraction statistics
   */
  getExtractionStats(tasks: ExtractedTask[]): {
    total: number;
    byAssignee: Record<string, number>;
    byPriority: Record<string, number>;
    byCategory: Record<string, number>;
    averageConfidence: number;
  } {
    const stats = {
      total: tasks.length,
      byAssignee: {} as Record<string, number>,
      byPriority: {} as Record<string, number>,
      byCategory: {} as Record<string, number>,
      averageConfidence: 0
    };

    tasks.forEach(task => {
      stats.byAssignee[task.assignee] = (stats.byAssignee[task.assignee] || 0) + 1;
      stats.byPriority[task.priority] = (stats.byPriority[task.priority] || 0) + 1;
      stats.byCategory[task.category] = (stats.byCategory[task.category] || 0) + 1;
    });

    stats.averageConfidence = tasks.reduce((sum, task) => sum + task.confidence, 0) / tasks.length;

    return stats;
  }
}

export default CallTranscriptionService;
export type { 
  CallRecording, 
  Transcription, 
  ExtractedTask, 
  TranscriptionSegment,
  SpeakerIdentification,
  TaskExtractionRules
};
